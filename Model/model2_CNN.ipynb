{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"collapsed_sections":["na_ZM2z-5W3x"],"authorship_tag":"ABX9TyNnjuSyLgz1me3Zzbh8htT1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Environment"],"metadata":{"id":"na_ZM2z-5W3x"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"J0ZnDCZw5T5I"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os\n","import time\n","import copy\n","import json\n","import math\n","import ast\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm import tqdm"],"metadata":{"id":"kIJg7UaW5nKA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# path\n","model_path =  \"/content/drive/MyDrive/AITermProject/Models/\"\n","name = str(int(time.time()))\n","save_path = os.path.join(model_path, name)\n","if not os.path.isdir(save_path):\n","  os.mkdir(save_path)\n","model_save_path = os.path.join(save_path, f\"{name}.pt\")\n","print('model_save_path:', model_save_path)\n","loss_path = os.path.join(save_path, \"loss.csv\")\n","\n","# feature file\n","feature_name = \"feature_0.2.csv\"\n","feature_save_path = f\"/content/drive/MyDrive/AITermProject/features/{feature_name}\""],"metadata":{"id":"0e_lQ-yd-SH1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["params = {\n","    \"BatchSize\": 64,\n","    \"Epochs\": 6,\n","    \"CountSteps\": 500,\n","    \"InputSize\": 1,\n","    \"OutputChannel\": 128,\n","    \"OutputSize\": 30,\n","    \"NumLayer\": 4,\n","    \"KerneiSize\": 3,\n","    \"LearningRate\": 0.0005,\n","    \"StepSize\": 3,\n","    \"Decay\": 5e-4,\n","    \"FcHiddenSize1\": 128,\n","    \"FcHiddenSize2\": 64,\n","    \"Dropout\": 0.3\n","}\n","\n","# write parameters\n","file_name = os.path.join(save_path, \"training_parameters.txt\")\n","with open(file_name, 'w') as file:\n","    for key, value in params.items():\n","        file.write(f\"{key}: {value}\\n\")\n","\n","print(f\"Parameters saved to {file_name}\")"],"metadata":{"id":"7fO1L7qV9Mqp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training Constants\n","BatchSize = params['BatchSize']\n","Epochs = params['Epochs']\n","CountSteps = params['CountSteps']\n","\n","# Model Constants\n","InputSize = params['InputSize']\n","OutputChannel = params['OutputChannel']\n","OutputSize = params['OutputSize']\n","NumLayer = params['NumLayer']\n","KerneiSize = params['KerneiSize']\n","LearningRate = params['LearningRate']\n","StepSize = params['StepSize']\n","Decay = params['Decay']\n","FcHiddenSize1 = params['FcHiddenSize1']\n","FcHiddenSize2 = params['FcHiddenSize2']\n","Dropout = params['Dropout']\n","\n","SeqLen = 0 # caltualted in feature\n","\n","# Other set\n","FeatureColumns = ['sbi_onehour', 'sno_value', 'act', 'tot', 'sbi', 'lat', 'lng', 'date_value',\n","                  'time', 'position','week', 'popularity', 'rainfall', 'see_rate_value']"],"metadata":{"id":"7dgnD4JX5gv9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data"],"metadata":{"id":"uFYdaeShQSe8"}},{"cell_type":"code","source":["features = pd.read_csv(feature_save_path)\n","\n","# sno normalizartion\n","min_sno = features['sno'].min()\n","features['sno_value'] = features['sno'] - min_sno\n","\n","print(\"data size:\", len(features))\n","print(features.head())\n","\n","# get data to split train and validation smaples\n","unique_dates = features['date'].unique()\n","unique_dates.sort()\n","print(\"All unique dates:\", unique_dates)\n","validation_dates = unique_dates[-3:]\n","print(\"Validation dates:\", validation_dates)\n","\n","validation_data = features[features['date'].isin(validation_dates)]\n","training_data = features[~features['date'].isin(validation_dates)]\n","\n","print(\"training samples:\", len(training_data))\n","print(\"validation samples\", len(validation_data))"],"metadata":{"collapsed":true,"id":"pHDaYmxg5Vo-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class YouBikeDataset(Dataset):\n","  def __init__(self, data, mean=None, std=None):\n","    self.data = data\n","    self.features = FeatureColumns\n","    self.target = 'sbi_prediction'\n","\n","    # Calculate mean and std if not provided\n","    if mean is None or std is None:\n","      non_time_data = self.data[self.features[1:]]\n","      self.mean = non_time_data.mean(axis=0).values\n","      self.std = non_time_data.std(axis=0).values\n","    else:\n","      self.mean = mean\n","      self.std = std\n","\n","  def __len__(self):\n","      return len(self.data)\n","\n","  def __getitem__(self, idx):\n","    row = self.data.iloc[idx]\n","    sbi_onehour = np.array(ast.literal_eval(row[self.features[0]]), dtype=np.float32)\n","    if sbi_onehour.ndim == 0:\n","      print(\"sbi_onehour\", row['sno'])\n","      sbi_onehour = np.expand_dims(sbi_onehour, axis=0)\n","\n","    non_time_features = np.array([row[feature] for feature in self.features[1:]], dtype=np.float32)\n","    if non_time_features.ndim == 0:\n","      print(\"non_time_features\", row['sno'])\n","      non_time_features = np.expand_dims(non_time_features, axis=0)\n","\n","    # Normalize non-time features\n","    non_time_features = (non_time_features - self.mean) / self.std\n","\n","    input_features = np.concatenate([sbi_onehour, non_time_features])\n","\n","    target = np.array(ast.literal_eval(row[self.target]), dtype=np.float32)\n","    return (torch.tensor(input_features, dtype=torch.float32).unsqueeze(0).transpose(0, 1),\n","            torch.tensor(target, dtype=torch.float32).unsqueeze(0).transpose(0, 1))\n","\n","# Calculate mean and std on training data\n","train_non_time_data = training_data[FeatureColumns[1:]]\n","mean = train_non_time_data.mean(axis=0).values\n","std = train_non_time_data.std(axis=0).values\n","\n","train_dataset = YouBikeDataset(training_data, mean, std)\n","val_dataset = YouBikeDataset(validation_data, mean, std)\n","\n","# DataLoader\n","train_loader = DataLoader(train_dataset, batch_size=BatchSize, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=BatchSize, shuffle=False)\n","\n","# check DataLoader\n","for inputs, targets in train_loader:\n","  print(\"Inputs:\", inputs.shape)  # (batch_size, seq_len, 1)\n","  print(list(inputs[0, :, 0]))\n","  SeqLen = inputs.shape[1]\n","  print('inputsize', InputSize)\n","  print(\"Targets:\", targets.shape)  # (batch_size, seq_len, 1)\n","  print(list(targets[0, :, 0]))\n","  break"],"metadata":{"id":"wgdrbPFKfJEF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model"],"metadata":{"id":"4lILt7DJPSBP"}},{"cell_type":"code","source":["class TemporalCNN(nn.Module):\n","  def __init__(self, input_channels, output_channels, num_layers=3, kernel_size=3):\n","    super(TemporalCNN, self).__init__()\n","    layers = []\n","    current_channels = input_channels\n","    for _ in range(num_layers):\n","      layers.append(nn.Conv1d(current_channels, output_channels, kernel_size, padding=kernel_size//2))\n","      layers.append(nn.BatchNorm1d(output_channels))\n","      layers.append(nn.ReLU())\n","      current_channels = output_channels\n","      output_channels = output_channels//2\n","    self.conv_layers = nn.Sequential(*layers)\n","\n","  def forward(self, x):\n","    x = x.transpose(1, 2)  # Conv1d expects (batch, channels, seq_len)\n","    x = self.conv_layers(x)\n","    x = x.transpose(1, 2)  # Transpose back to (batch, seq_len, channels)\n","    x = x.flatten(start_dim=1)  # Flatten the output\n","    return x"],"metadata":{"id":"UAH_tC1VrmQx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TemporalCNNWithFC(nn.Module):\n","    def __init__(self, input_channels, output_channels, num_layers=3, kernel_size=3,\n","                 fc_hidden_size1=128, fc_hidden_size2=64, dropout_p=0.5, output_size=30,\n","                 sequence_length=43):\n","        super(TemporalCNNWithFC, self).__init__()\n","        self.temporal_cnn = TemporalCNN(input_channels, output_channels, num_layers, kernel_size)\n","        final_output_channels = output_channels // (2 ** (num_layers - 1))  # output_channels of t-cnn\n","        fc_insize = final_output_channels * sequence_length\n","        self.fc1 = nn.Linear(fc_insize, fc_hidden_size1)\n","        self.dropout1 = nn.Dropout(dropout_p)\n","        self.fc2 = nn.Linear(fc_hidden_size1, fc_hidden_size2)\n","        self.dropout2 = nn.Dropout(dropout_p)\n","        self.fc3 = nn.Linear(fc_hidden_size2, output_size)\n","\n","    def forward(self, x):\n","        x = self.temporal_cnn(x)  # (batch_size, seq_len, output_channels)\n","        x = F.relu(self.fc1(x))\n","        x = self.dropout1(x)\n","        x = F.relu(self.fc2(x))\n","        x = self.dropout2(x)\n","        x = self.fc3(x)  # (batch_size, seq_len, output_size)\n","        return x.unsqueeze(1)  # (batch_size, 1, output_size)"],"metadata":{"id":"fjskQWbYdSzD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = TemporalCNNWithFC(InputSize,\n","                          OutputChannel,\n","                          num_layers=NumLayer,\n","                          kernel_size=KerneiSize,\n","                          output_size=OutputSize,\n","                          sequence_length = SeqLen,\n","                          fc_hidden_size1=FcHiddenSize1,\n","                          fc_hidden_size2=FcHiddenSize2,\n","                          dropout_p=Dropout\n","                          )\n","\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=LearningRate, weight_decay=Decay)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=StepSize, gamma=0.1)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)"],"metadata":{"id":"yoMLciiKy8kT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"fmvSvFBTv483"}},{"cell_type":"code","source":["def write_losses(loss_path, train_losses, dev_losses):\n","  losses={}\n","  losses[\"train\"] = train_losses\n","  losses[\"test\"] = dev_losses\n","  with open(loss_path, \"w\") as out_config:\n","    json.dump(losses, out_config, indent=4)"],"metadata":{"id":"oFPFIS-13gQC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_losses = []\n","test_losses = []\n","best_loss = float('inf')\n","\n","for epoch in range(Epochs):\n","  print(f\"[Training] Epoch {epoch}/{Epochs - 1}\")\n","  print(\"-\" * 10)\n","\n","  # Train\n","  model.train()\n","  running_loss_train = 0.0\n","  intermediate_loss = 0.0\n","  for count, (features, targets) in enumerate(tqdm(train_loader)):\n","    features, targets = features.to(device), targets.to(device)\n","    optimizer.zero_grad()\n","    outputs = model(features)\n","    loss = criterion(outputs, targets)\n","    loss.backward()\n","    optimizer.step()\n","    running_loss_train += loss.item()\n","    intermediate_loss += loss.item()\n","\n","    # Print training losses after CountSteps\n","    if count % CountSteps == 0 and count != 0:\n","      average_loss = intermediate_loss / CountSteps\n","      print(f\"Loss in epoch {epoch} - step {count}: {average_loss:.4f}\")\n","      train_losses.append(average_loss)\n","      intermediate_loss = 0.0\n","\n","  scheduler.step()\n","\n","  epoch_train_loss = running_loss_train / len(train_loader)\n","  print(f\"Training Loss: {epoch_train_loss:.4f}\")\n","\n","  # Evaluation\n","  model.eval()\n","  running_loss_test = 0.0\n","  with torch.no_grad():\n","    for features, targets in val_loader:\n","      features, targets = features.to(device), targets.to(device)\n","      outputs = model(features)\n","      loss = criterion(outputs, targets)\n","      running_loss_test += loss.item()\n","\n","  epoch_test_loss = running_loss_test / len(val_loader)\n","  test_losses.append(epoch_test_loss)\n","  print(f\"Validation Loss: {epoch_test_loss:.4f}\")\n","\n","  if epoch_test_loss < best_loss:\n","    write_losses(loss_path, train_losses, test_losses)\n","    best_loss = epoch_test_loss\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    torch.save(best_model_wts, model_save_path)\n","    print(f\"Updated best model on dev checkpoint: {model_save_path}\")"],"metadata":{"id":"2Mdh3Tiq-qSp"},"execution_count":null,"outputs":[]}]}