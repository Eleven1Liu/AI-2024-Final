{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyOCWWHHA1dymA+gVVUCg6Ip"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Environment"],"metadata":{"id":"nTQNwTpIzHRc"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"JN36eKmiy0xI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import json\n","import zipfile\n","import pandas as pd\n","from datetime import timedelta"],"metadata":{"id":"Tou8SLyvyrAI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["popularity_data = {\n","    '捷運公館站(2號出口)': 500101022,\n","    '捷運公館站(3號出口)': 500101181,\n","    '捷運科技大樓站': 500101001,\n","    '捷運芝山站(2號出口)_1': 500104108,\n","    '捷運中山國中站': 500107024,\n","    '捷運永春站(2號出口)': 500112070,\n","    '捷運圓山站(1號出口)': 500103009,\n","    '捷運六張犁站': 500101101,\n","    '捷運台北101/世貿站(2號出口)': 500112076,\n","    '捷運行天宮站(3號出口)': 500107036\n","}\n","\n","rainfall_data = {\n","    '2024-04-25': 10.5,\n","    '2024-04-26': 29.5,\n","    '2024-04-27': 0.5,\n","    '2024-04-28': 1.0,\n","    '2024-04-29': 0.0,\n","    '2024-04-30': 4.5,\n","    '2024-05-01': 7.5,\n","    '2024-05-02': 9.5,\n","    '2024-05-03': 0.0,\n","    '2024-05-04': 0.1,\n","    '2024-05-05': 0.1,\n","    '2024-05-06': 0.0,\n","    '2024-05-07': 0.0,\n","    '2024-05-08': 0.0,\n","    '2024-05-09': 0.0,\n","    '2024-05-10': 0.0,\n","    '2024-05-11': 0.0,\n","    '2024-05-12': 16.0,\n","    '2024-05-13': 14.0,\n","    '2024-05-14': 0.5,\n","    '2024-05-15': 0.0,\n","    '2024-05-16': 0.0,\n","    '2024-05-17': 0.0,\n","    '2024-05-18': 0.0,\n","    '2024-05-19': 0.0,\n","    '2024-05-20': 0.0,\n","    '2024-05-21': 0.0,\n","    '2024-05-22': 0.0,\n","    '2024-05-23': 0.0,\n","    '2024-05-24': 0.0,\n","    '2024-05-25': 0.1\n","}\n","\n","START=\"2024-04-25\"\n","END=\"2024-05-19\"\n","INTERVAL = 2\n","DURATION = 60 // INTERVAL\n","\n","# TMPDATE= 500101001\n","TMPDATE = \"\"\n","RATE = 1\n","\n","data_name_0513_0519 = \"data_0513_0519_2mins.zip\"\n","data_name_0506_0512 = \"data_0506_0512_2mins.zip\"\n","\n","\n","feature_name = f\"feature_{TMPDATE}_{RATE}\"\n","\n","data_path_0506_0512 = f\"/content/drive/MyDrive/AITermProject/{data_name_0506_0512}\"\n","data_path_0513_0519 = f\"/content/drive/MyDrive/AITermProject/{data_name_0513_0519}\"\n","see_rate_path = \"/content/drive/MyDrive/AITermProject/see_rate.csv\"\n","mrt_distance_path = \"/content/drive/MyDrive/AITermProject/mrt_ubike_shortest_dist.csv\"\n","\n","feature_save_path = f\"/content/drive/MyDrive/AITermProject/{feature_name}.csv\"\n","day_seq_path = f\"/content/drive/MyDrive/AITermProject/day_{TMPDATE}.txt\""],"metadata":{"id":"CQCxsd-yayli"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data"],"metadata":{"id":"qBPTFVRq10F5"}},{"cell_type":"code","source":["def data_reading(data_path):\n","  with zipfile.ZipFile(data_path, 'r') as z:\n","    z.printdir()\n","    csv_filename = z.namelist()[0]\n","    with z.open(csv_filename) as f:\n","      df = pd.read_csv(f)\n","    return df"],"metadata":{"id":"VxAdKru1tL2o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_0506_0512 = data_reading(data_path_0506_0512)\n","df_0513_0519 = data_reading(data_path_0513_0519)\n","df = pd.concat([df_0506_0512, df_0513_0519], axis=0, ignore_index=True)\n","\n","if TMPDATE:\n","  df = df[df['sno'] == TMPDATE]\n","  df.to_csv(f\"/content/drive/MyDrive/AITermProject/{TMPDATE}.csv\")\n","\n","# keep columns in need\n","columns_to_keep = ['sno', 'updateTime', 'act', 'total', 'available_rent_bikes', 'latitude', 'longitude']\n","# columns_to_keep = ['sno', 'updateTime', 'act', 'tot', 'sbi', 'lat', 'lng']\n","df = df[columns_to_keep]\n","\n","# Rename columns\n","columns_to_rename = {\n","    'total': 'tot',\n","    'available_rent_bikes': 'sbi',\n","    'latitude': 'lat',\n","    'longitude': 'lng'\n","    }\n","df = df.rename(columns=columns_to_rename)\n","\n","# Count total rows\n","total_rows = len(df)\n","print(f\"Total rows: {total_rows}\")\n","nan_rows_count = df.isna().sum().sum()\n","print(f\"Rows with NaN values: {nan_rows_count}\")\n","df = df.dropna()\n","print(df.head())"],"metadata":{"id":"Dv-2EVQ4tnxP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title sequence time data\n","\n","# data features\n","df['updateTime'] = pd.to_datetime(df['updateTime'])\n","df['date'] = df['updateTime'].dt.date\n","df['date_value'] = df['date'].apply(lambda x: int(x.strftime('%m%d')))\n","\n","# # data position in time sequence\n","# df['position'] = df['updateTime'].dt.hour * (60//INTERVAL) + df['updateTime'].dt.minute // INTERVAL\n","# print(df.head())\n","\n","# day_point = {}\n","# for i, row in df.iterrows():\n","#   key = f\"{row['sno']}_{row['date'].strftime('%Y-%m-%d')}\"\n","#   if key not in day_point:\n","#     day_point[key] = [-1] * (24 * DURATION)\n","#   position = row['position']\n","#   day_point[key][position] = row['sbi']\n","# print('length of key list:', len(day_point))\n","\n","# # Save day_point as a text file\n","# with open(os.path.join(day_seq_path), 'w') as file:\n","#     json.dump(day_point, file)\n","# print(\"day_point has been saved.\")"],"metadata":{"id":"htsExpFnn_Mf","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title mask\n","def generate_padding_mask(sequence, pad_value=-1):\n","    return [x == pad_value for x in sequence]"],"metadata":{"id":"N_3TA9lxPY92"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # @title get one-hour-before and day-before history data as features\n","\n","# df = df.sample(frac=RATE, random_state=42)\n","\n","# df['sbi_onehour'] = None\n","# df['sbi_history'] = None\n","# df['sbi_prediction'] = None\n","# # mask for padded position with 1\n","# df['sbi_onehour_mask'] = None\n","# df['sbi_history_mask'] = None\n","# df['sbi_prediction_mask'] = None\n","\n","# for i, row in df.iterrows():\n","#   current_date = row['date']\n","#   pred_position = row['position'] + 1\n","#   sno = row['sno']\n","#   key = f\"{sno}_{current_date.strftime('%Y-%m-%d')}\"\n","\n","#   # to get the precdiction data\n","#   if pred_position + DURATION < 24 * DURATION :\n","#     sbi_prediction = day_point[key][pred_position:pred_position+DURATION]\n","#   else:\n","#     sbi_prediction = day_point[key][pred_position:]\n","#     remaining_points = DURATION - len(sbi_prediction)\n","#     next_day_key = f\"{sno}_{(current_date + pd.Timedelta(days=1)).strftime('%Y-%m-%d')}\"\n","#     if next_day_key in day_point and remaining_points > 0:\n","#       sbi_prediction += day_point[next_day_key][:remaining_points]\n","#     else:\n","#       sbi_prediction += [-1] * remaining_points\n","\n","#   # to get the previous one hour data\n","#   if pred_position >= DURATION:\n","#     sbi_onehour = day_point[key][pred_position-DURATION:pred_position]\n","#   else:\n","#     prev_day_key = f\"{sno}_{(current_date - pd.Timedelta(days=1)).strftime('%Y-%m-%d')}\"\n","#     if prev_day_key in day_point:\n","#       sbi_onehour = day_point[prev_day_key][-(DURATION-pred_position):] + day_point[key][:pred_position]\n","#     else:\n","#       sbi_onehour = [-1] * (DURATION - pred_position) + day_point[key][:pred_position]\n","\n","#   # to get the history data\n","#   sbi_history_list = []\n","#   sbi_history_mask = []\n","#   previous_days = [current_date - timedelta(days=i) for i in range(1, 6)] # last 5 days\n","#   for previous_day in previous_days:\n","#     history_key = f\"{sno}_{previous_day.strftime('%Y-%m-%d')}\"\n","#     if history_key in day_point:\n","#       start_pos = max(0, pred_position - DURATION)\n","#       end_pos = min(24 * DURATION, pred_position + DURATION)\n","#       sbi_history = day_point[history_key][start_pos:end_pos]\n","#       next_day_key = f\"{sno}_{(previous_day + timedelta(days=1)).strftime('%Y-%m-%d')}\"\n","#       prev_day_key = f\"{sno}_{(previous_day - timedelta(days=1)).strftime('%Y-%m-%d')}\"\n","#       if len(sbi_history) < 2 * DURATION:\n","#         if pred_position - DURATION < 0:\n","#           gap = DURATION - pred_position\n","#           if prev_day_key in day_point:\n","#             sbi_history = day_point[prev_day_key][-gap:] + sbi_history\n","#           else:\n","#             sbi_history = [-1] * (gap) + sbi_history\n","#         if pred_position + DURATION > 24 * DURATION:\n","#           gap = pred_position + DURATION - 24 * DURATION\n","#           if next_day_key in day_point:\n","#             sbi_history += day_point[next_day_key][:gap]\n","#           else:\n","#             sbi_history += [-1] * (gap)\n","#       sbi_history_list.append(sbi_history)\n","#       sbi_history_mask.append(generate_padding_mask(sbi_history))\n","\n","#   # store sequence in dataframe\n","#   df.at[i, 'sbi_prediction'] = sbi_prediction\n","#   df.at[i, 'sbi_onehour'] = sbi_onehour\n","#   df.at[i, 'sbi_history'] = sbi_history_list\n","#   df.at[i, 'sbi_prediction_mask'] = generate_padding_mask(sbi_prediction)\n","#   df.at[i, 'sbi_onehour_mask'] = generate_padding_mask(sbi_onehour)\n","#   df.at[i, 'sbi_history_mask'] = sbi_history_mask\n","\n","print(df.head())"],"metadata":{"id":"VeuPsEyq_f-f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title get simple features\n","\n","# time feature\n","df['time'] = df['updateTime'].dt.hour * 100 + df['updateTime'].dt.minute\n","\n","# weekend feature\n","date_range = pd.date_range(START, END)\n","date_to_week = {date: 1 if date.weekday() < 5 else 0 for date in date_range}\n","df['week'] = df['date'].map(date_to_week)\n","\n","# popularity\n","sno_to_popularity = {sno: 10 - i for i, sno in enumerate(popularity_data.values())}\n","df['popularity'] = df['sno'].map(sno_to_popularity).fillna(0).astype(int)\n","\n","# rainfall\n","date_to_rainfall = {pd.to_datetime(date): rainfall for date, rainfall in rainfall_data.items()}\n","df['rainfall'] = df['date'].map(date_to_rainfall)\n","\n","# see rate\n","see_rate = pd.read_csv(see_rate_path)\n","df = df.merge(see_rate[['sno', 'value']], on='sno', how='left')\n","df.rename(columns={'value': 'see_rate_value'}, inplace=True)\n","df['see_rate_value'].fillna(-1, inplace=True)\n","\n","# 距离数据\n","distances = pd.read_csv(mrt_distance_path)\n","df = df.merge(distances[['sno', 'mrt_distances']], on='sno', how='left')\n","df['mrt_distances'].fillna(-1, inplace=True)\n","\n","print(df.head())\n","df.to_csv(feature_save_path, index=False)"],"metadata":{"collapsed":true,"id":"UvJMktrXAgbp"},"execution_count":null,"outputs":[]}]}